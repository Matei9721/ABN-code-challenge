# ABN Interview Assignment
This repository contains the code solution to the ABN assignment.

### Overview

This repository contains a simple Spark application that does data loading, transformation and exporting
to the format required by **KommatiPara**. The goal of this application is to help the fictive stakeholders
from **KommatiPara** to combine different datasets about their clients in the aim of helping their marketing team.

A high level overview of the application can be summarized as following:
1. Load **KommatiPara** client datasets into Spark.
2. User input filtering for the countries.
3. Removal of Personal Identifiable Information (PII).
4. Column renaming for better readability.
5. Both datasets are joined and exported to local storage for further usages.

The application can be packaged and used as part of a bigger system, or it can be individually used by
running the script inside the main.py class which accept terminal arguments for customization. The main.py
script can be run as follows:

```python
 python main.py -c "Netherlands" "United Kingdom" -d1 "../data/input/dataset_one.csv"
 -d2 "../data/input/dataset_two.csv"
```

The repository also makes usage of GitHub Actions to set up an automated build pipeline
that makes sure the code follows all Python PEP 8 style guides and the tests implemented
pass successfully. The pipeline is set to automatically run on every push to Master
branch and also on every PR enabling CI/CD.

### Project Structure

- `main.py`: The main Python script for running the task of filtering and merging the datasets for The Netherlands and UK..
- `src/`: Directory containing main PySpark transformer and pipeline code.
  - `custom_logging.py`: Python file containing the Singleton Logger class used throughout the project.
  - `data_processing.py`: Python file containing the main logic for filtering and merging the client's datasets.
  - `transformers.py`: Python file containing the custom transformer PySpark classes used for data transformations.
- `tests/`: Directory containing main PySpark transformer and pipeline code.
- `requirements.txt`: A file listing the dependencies required for the project.
- `logs/`: A directory containing the logger output.
- `data/`: A directory containing the client's input data.
- `client_data/`: A directory for storing the output data generated by the main.py script.
- `.github/`: A directory containing the GitHub action workflow configuration file.

### Limitations / Future Improvements

- Improved error handling: Currently there is a limit amount of error handling implemented because of lack of time.
- Better code structure/refactor: The structure of the code is not that well done as it is quite hard to scale it up.
- More configurability: It would be nice to allow the client to input more than the files and countries to filter by.
